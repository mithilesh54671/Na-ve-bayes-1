{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Q1. What is Bayes' theorem?"
      ],
      "metadata": {
        "id": "gAS78nVI2bq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayes' theorem is a fundamental concept in probability theory and statistics that provides a framework for updating probability beliefs or estimates based on new evidence or information. It's named after the 18th-century statistician and philosopher Thomas Bayes.\n",
        "\n",
        "The core idea of Bayes' theorem is to calculate the probability of an event (A) occurring given that another event (B) has occurred.\n",
        "\n",
        "This conditional probability, denoted as P(A|B), is calculated using the following formula:\n",
        "\n",
        "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
        "\n",
        "Where:\n",
        "\n",
        "P(A|B) represents the conditional probability of event A occurring given that event B has occurred.\n",
        "P(B|A) is the conditional probability of event B occurring given that event A has occurred.\n",
        "P(A) is the prior probability of event A occurring independently of any information about event B.\n",
        "P(B) is the prior probability of event B occurring independently of any information about event A.\n",
        "Bayes' theorem allows us to update our belief or estimate of the probability of an event (A) occurring based on new evidence or information (B). It is commonly used in various fields, including machine learning, statistics, and data science, for tasks such as Bayesian inference, spam filtering, medical diagnosis, and more. Bayesian statistics, in particular, is a branch of statistics that relies heavily on Bayes' theorem to update probability distributions based on data and prior beliefs."
      ],
      "metadata": {
        "id": "x8QjMBj72hnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q2. What is the formula for Bayes' theorem?"
      ],
      "metadata": {
        "id": "auxFqTzq2oUV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
        "\n",
        "Where:\n",
        "\n",
        "P(A|B) represents the conditional probability of event A occurring given that event B has occurred. This is the probability we want to calculate.\n",
        "P(B|A) is the conditional probability of event B occurring given that event A has occurred.\n",
        "P(A) is the prior probability of event A occurring independently of any information about event B.\n",
        "P(B) is the prior probability of event B occurring independently of any information about event A."
      ],
      "metadata": {
        "id": "Xcculwb32wDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q3. How is Bayes' theorem used in practice?"
      ],
      "metadata": {
        "id": "iGN7jOP92xqU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Say we have a set of independent features age,height and weight and overweight as the output feature,\n",
        "\n",
        "Here age,height and weight are independent feature and overweight is the dependent feature\n",
        "\n",
        "age   height   weight   overweight\n",
        "17     170       73         no\n",
        "21     165       91         yes\n",
        "28     178       88         yes\n",
        "42     160       60         no\n",
        "\n",
        "Now we have to find if the pearson is overweight or not based on age height and weight,\n",
        "\n",
        "Considering,\n",
        "\n",
        "Age,height and weight as x1,x2 amd x3 respectively and\n",
        "overweight as y\n",
        "By Using Bayes Theorem ,\n",
        "\n",
        "P(y/(x1,x2,x3) = (P(y) * P(x1,x2,x3)/y ) / P(x1,x2,x3)\n",
        "P(y/(x1,x2,x3) = [(P(y) * P(x1/y) * P(x2/y) * P(x3)/y ] / P(x1,x2,x3)\n",
        "P(yes/(x1,x2,x3) = [(P(yes) * P(x1/yes) * P(x2/yes) * P(x3/yes) ] / P(x1,x2,x3)\n",
        "P(no/(x1,x2,x3) = [(P(no) * P(x1/no) * P(x2/no) * P(x3/no) ] / P(x1,x2,x3)\n",
        "\n",
        "The Denominator P(x1,x2,x3) is contant and can be ignored in calculation\n",
        "\n",
        "Final Formula is ,\n",
        "\n",
        "P(yes/(x1,x2,x3) = (P(yes) * P(x1/yes) * P(x2/yes) * P(x3/yes)\n",
        "P(no/(x1,x2,x3) = (P(no) * P(x1/no) * P(x2/no) * P(x3/no)\n",
        "\n",
        "The one whose probabilty is the highest is considered the output."
      ],
      "metadata": {
        "id": "IPctlIBJ21Lo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q4. What is the relationship between Bayes' theorem and conditional probability?"
      ],
      "metadata": {
        "id": "zos79vtO23du"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayes' theorem is a mathematical formula that describes how to calculate conditional probabilities, and it provides a framework for updating probability beliefs based on new information or evidence.\n",
        "\n",
        "The relationship between Bayes' theorem and conditional probability is as follows:\n",
        "\n",
        "Conditional Probability: Conditional probability is a fundamental concept that deals with the probability of an event occurring given that another event has already occurred. It's denoted as P(A|B), which represents the probability of event A occurring given that event B has occurred.\n",
        "\n",
        "Bayes' Theorem: Bayes' theorem is a specific formula that allows you to calculate conditional probabilities. It relates the conditional probability P(A|B) to other probabilities, including the prior probability of A (P(A)), the prior probability of B (P(B)), and the likelihood of B given A (P(B|A)). The formula is as follows:\n",
        "\n",
        "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
        "\n",
        "P(A|B) is the conditional probability you want to calculate, and it's related to the other probabilities in the equation.\n",
        "Bayes' theorem is a mathematical tool for calculating conditional probabilities, and it formalizes the process of updating probability beliefs in light of new evidence. It's a fundamental concept in Bayesian probability and statistics, and it plays a crucial role in various applications, such as machine learning, and decision-making under uncertainty."
      ],
      "metadata": {
        "id": "CYApqWmp28_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?"
      ],
      "metadata": {
        "id": "aYVcOGdL2_FQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 3 Types of Naive Bayes classifiers :\n",
        "\n",
        "Gaussian Naive Bayes (GNB):\n",
        "\n",
        "Data Type: GNB is suitable for continuous or real-valued data.\n",
        "Assumptions: It assumes that the features follow a Gaussian (normal) distribution.\n",
        "Example Applications: GNB is often used for problems involving continuous features, such as spam detection (with word frequencies as features) or medical diagnosis (with physiological measurements).\n",
        "Multinomial Naive Bayes (MNB):\n",
        "\n",
        "Data Type: MNB is commonly used with discrete data, especially when dealing with text data.\n",
        "Assumptions: It assumes that the features represent the counts or frequencies of events (e.g., word counts in text documents).\n",
        "Example Applications: MNB is well-suited for text classification problems like sentiment analysis or document categorization, where features often represent word counts or term frequencies.\n",
        "Bernoulli Naive Bayes (BNB):\n",
        "\n",
        "Data Type: BNB is also used with discrete data, but it's tailored for binary features (i.e., features that are either present or absent).\n",
        "Assumptions: It assumes that features are binary, where 1 represents the presence of a feature and 0 represents the absence.\n",
        "Example Applications: BNB is commonly applied in text classification tasks where binary features indicate whether specific words appear in a document or not (e.g., spam or not spam classification based on the presence of certain words)."
      ],
      "metadata": {
        "id": "95WUsY0V3E1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q6. Assignment: You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of each feature value for each class:"
      ],
      "metadata": {
        "id": "v9nNZl5i3HIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class\tX1=1\tX1=2\tX1=3\tX2=1\tX2=2\tX2=3\tX2=4\n",
        "\n",
        "A\t3\t3\t4\t4\t3\t3\t3\n",
        "B\t2\t2\t1\t2\t2\t2\t3\n",
        "\n",
        "Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance\n",
        "to belong to?"
      ],
      "metadata": {
        "id": "BPyyYtIT3T5a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's denote:\n",
        "\n",
        "P(A) as the prior probability of class A (given as equal to P(B) in this case because of equal prior probabilities for each class).\n",
        "P(X1 = 3 | A) as the conditional probability of observing X1 = 3 given class A.\n",
        "P(X2 = 4 | A) as the conditional probability of observing X2 = 4 given class A.\n",
        "P(X1 = 3 | B) as the conditional probability of observing X1 = 3 given class B.\n",
        "P(X2 = 4 | B) as the conditional probability of observing X2 = 4 given class B.\n",
        "You can calculate these conditional probabilities based on the provided frequency table:\n",
        "\n",
        "P(X1 = 3 | A) = 4 / (4 + 3) = 4/7\n",
        "P(X2 = 4 | A) = 3 / (4 + 3) = 3/7\n",
        "P(X1 = 3 | B) = 1 / (1 + 2) = 1/3\n",
        "P(X2 = 4 | B) = 3 / (2 + 2 + 3) = 3/7\n",
        "Now, you can use Naive Bayes to calculate the posterior probabilities for each class:\n",
        "\n",
        "For class A:\n",
        "\n",
        "P(A | X1 = 3, X2 = 4) = P(X1 = 3 | A) * P(X2 = 4 | A) * P(A)\n",
        "                                 = (4/7) * (3/7) * (1/2)\n",
        "                                  = 6/98\n",
        "                                  = 3/49\n",
        "                                  = 0.06122\n",
        "For class B:\n",
        "\n",
        "P(B | X1 = 3, X2 = 4) = P(X1 = 3 | B) * P(X2 = 4 | B) * P(B)\n",
        "                                 = (1/3) * (3/7) * (1/2)\n",
        "                                  = 1/14\n",
        "                                  = 0.07142\n",
        "The Probabilty of class Prediction is calculated as,\n",
        "\n",
        "P(A | X1 = 3, X2 = 4) = 0.06122 / (0.06122 + 0.07142)\n",
        "                                 = 0.4615 * 100\n",
        "                                 = 46.15%\n",
        "\n",
        "P(B | X1 = 3, X2 = 4) = 07142 / (0.06122 + 0.07142)\n",
        "                                 = 0.5384 * 100\n",
        "                                 = 53.84%\n",
        "\n",
        "Since Probability of Class B is 53.84 which is greater then Class A of 46.15 , Naive Bayes would predict that the new instance with features X1 = 3 and X2 = 4 belongs to class B."
      ],
      "metadata": {
        "id": "WtGHbtA13nBL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScjSCzQu2YKM"
      },
      "outputs": [],
      "source": []
    }
  ]
}